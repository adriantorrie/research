{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Following Strategy\n",
    "\n",
    "This notebook answers question 3.4 form the text book Advances in Financial Machine Learning.\n",
    "\n",
    "3.4 Develop a trend-following strategy based on a popular technical analysis statistic (e.g., crossing moving averages). For each observation, the model suggests a side, but not a size of the bet.\n",
    "\n",
    "* (a) Derive meta-labels for ptSl = [1, 2] and t1 where numDays = 1. Use as trgt the daily standard deviation as computed by Snippet 3.1.\n",
    "* (b) Train a random forest to decide whether to trade or not. Note: The decision is whether to trade or not, {0,1}, since the underlying model (the crossing moving average) has decided the side, {-1, 1}.\n",
    "\n",
    "I took some liberties by extending the features to which I use to build the meta model. I also add some performance metrics at the end. \n",
    "\n",
    "In conclusion: Meta Labeling works, SMA strategies suck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Hudson and Thames MlFinLab package\n",
    "import mlfinlab as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyfolio as pf\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#mlfinlab imports\n",
    "from mlfinlab.labeling import add_vertical_barrier, get_bins, get_events\n",
    "from mlfinlab.filters import cusum_filter\n",
    "from mlfinlab.util import get_daily_vol\n",
    "from mlfinlab.sampling.bootstrapping import get_ind_matrix, seq_bootstrap\n",
    "from mlfinlab.ensemble.forest import SequentialBootstrappingRandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "We are using the dollar bars based off of the high quality HFT data we purchased. There is a sample of bars available in this branch as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv('official_data/dollar_bars.csv')\n",
    "data.index = pd.to_datetime(data['date_time'])\n",
    "data = data.drop('date_time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['2011-09-01':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Fit a Primary Model: Trend Following\n",
    "Based on the simple moving average cross-over strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute moving averages\n",
    "fast_window = 20\n",
    "slow_window = 50\n",
    "\n",
    "data['fast_mavg'] = data['close'].rolling(window=fast_window, min_periods=fast_window, center=False).mean()\n",
    "data['slow_mavg'] = data['close'].rolling(window=slow_window, min_periods=slow_window, center=False).mean()\n",
    "data.head()\n",
    "\n",
    "# Compute sides\n",
    "data['side'] = np.nan\n",
    "\n",
    "long_signals = data['fast_mavg'] >= data['slow_mavg'] \n",
    "short_signals = data['fast_mavg'] < data['slow_mavg'] \n",
    "data.loc[long_signals, 'side'] = 1\n",
    "data.loc[short_signals, 'side'] = -1\n",
    "\n",
    "# Remove Look ahead biase by lagging the signal\n",
    "data['side'] = data['side'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw data\n",
    "raw_data = data.copy()\n",
    "\n",
    "# Drop the NaN values from our data set\n",
    "data.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['side'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Events: CUSUM Filter\n",
    "Predict what will happen when a CUSUM event is triggered. Use the signal from the MAvg Strategy to determine the side of the bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute daily volatility\n",
    "daily_vol = get_daily_vol(close=data['close'], lookback=50)\n",
    "\n",
    "# Apply Symmetric CUSUM Filter and get timestamps for events\n",
    "# Note: Only the CUSUM filter needs a point estimate for volatility\n",
    "cusum_events = cusum_filter(data['close'], threshold=daily_vol['2011-09-01':'2018-01-01'].mean()*0.5)\n",
    "\n",
    "# Compute vertical barrier\n",
    "vertical_barriers = add_vertical_barrier(t_events=cusum_events, close=data['close'], num_days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_sl = [1, 2]\n",
    "min_ret = 0.005\n",
    "triple_barrier_events = get_events(close=data['close'],\n",
    "                                               t_events=cusum_events,\n",
    "                                               pt_sl=pt_sl,\n",
    "                                               target=daily_vol,\n",
    "                                               min_ret=min_ret,\n",
    "                                               num_threads=3,\n",
    "                                               vertical_barrier_times=vertical_barriers,\n",
    "                                               side_prediction=data['side'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_bins(triple_barrier_events, data['close'])\n",
    "labels.side.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Results of Primary Model:\n",
    "What is the accuracy of predictions from the primary model (i.e., if the sec- ondary model does not filter the bets)? What are the precision, recall, and F1-scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_forecast = pd.DataFrame(labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "# Performance Metrics\n",
    "actual = primary_forecast['actual']\n",
    "pred = primary_forecast['pred']\n",
    "print(classification_report(y_true=actual, y_pred=pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(actual, pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(actual, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few takeaways**\n",
    "* There is an imbalance in the classes - more are classified as \"no trade\"\n",
    "* Meta-labeling says that there are many false-positives  \n",
    "* the sklearn's confusion matrix is [[TN, FP][FN, TP]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fit a Meta Model (Standard Random Forest)\n",
    "Train a random forest to decide whether to trade or not (i.e 1 or 0 respectively) since the earlier model has decided the side (-1 or 1)\n",
    "\n",
    "Create the following features: \n",
    "* Volatility\n",
    "* Serial Correlation\n",
    "* The returns at the different lags from the serial correlation\n",
    "* The sides from the SMavg Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['close']).diff()\n",
    "\n",
    "# Momentum\n",
    "raw_data['mom1'] = raw_data['close'].pct_change(periods=1)\n",
    "raw_data['mom2'] = raw_data['close'].pct_change(periods=2)\n",
    "raw_data['mom3'] = raw_data['close'].pct_change(periods=3)\n",
    "raw_data['mom4'] = raw_data['close'].pct_change(periods=4)\n",
    "raw_data['mom5'] = raw_data['close'].pct_change(periods=5)\n",
    "\n",
    "# Volatility\n",
    "raw_data['volatility_50'] = raw_data['log_ret'].rolling(window=50, min_periods=50, center=False).std()\n",
    "raw_data['volatility_31'] = raw_data['log_ret'].rolling(window=31, min_periods=31, center=False).std()\n",
    "raw_data['volatility_15'] = raw_data['log_ret'].rolling(window=15, min_periods=15, center=False).std()\n",
    "\n",
    "# Serial Correlation (Takes about 4 minutes)\n",
    "window_autocorr = 50\n",
    "\n",
    "raw_data['autocorr_1'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "raw_data['autocorr_2'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=2), raw=False)\n",
    "raw_data['autocorr_3'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=3), raw=False)\n",
    "raw_data['autocorr_4'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=4), raw=False)\n",
    "raw_data['autocorr_5'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr, center=False).apply(lambda x: x.autocorr(lag=5), raw=False)\n",
    "\n",
    "# Get the various log -t returns\n",
    "raw_data['log_t1'] = raw_data['log_ret'].shift(1)\n",
    "raw_data['log_t2'] = raw_data['log_ret'].shift(2)\n",
    "raw_data['log_t3'] = raw_data['log_ret'].shift(3)\n",
    "raw_data['log_t4'] = raw_data['log_ret'].shift(4)\n",
    "raw_data['log_t5'] = raw_data['log_ret'].shift(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re compute sides\n",
    "raw_data['side'] = np.nan\n",
    "\n",
    "long_signals = raw_data['fast_mavg'] >= raw_data['slow_mavg']\n",
    "short_signals = raw_data['fast_mavg'] < raw_data['slow_mavg']\n",
    "\n",
    "raw_data.loc[long_signals, 'side'] = 1\n",
    "raw_data.loc[short_signals, 'side'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove look ahead bias\n",
    "raw_data = raw_data.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now get the data at the specified events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features at event dates\n",
    "X = raw_data.loc[labels.index, :]\n",
    "\n",
    "# Drop unwanted columns\n",
    "X.drop(['open', 'high', 'low', 'close', 'cum_vol', 'cum_dollar', 'cum_ticks','fast_mavg', 'slow_mavg',], axis=1, inplace=True)\n",
    "\n",
    "y = labels['bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train/validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation and test sets\n",
    "X_training_validation = X['2011-09-01':'2018-01-01']\n",
    "y_training_validation = y['2011-09-01':'2018-01-01']\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_training_validation, y_training_validation, test_size=0.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[2, 3, 4, 5, 7],\n",
    "              'n_estimators':[100, 256, 512],\n",
    "              'random_state':[42]}\n",
    "    \n",
    "def perform_grid_search(X_data, y_data):\n",
    "    rf = RandomForestClassifier(criterion='entropy', class_weight='balanced_subsample')\n",
    "    \n",
    "    clf = GridSearchCV(rf, parameters, cv=4, scoring='roc_auc', n_jobs=3)\n",
    "    \n",
    "    clf.fit(X_data, y_data)\n",
    "    \n",
    "    print(clf.cv_results_['mean_test_score'])\n",
    "    \n",
    "    return clf.best_params_['n_estimators'], clf.best_params_['max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract parameters\n",
    "n_estimator, depth = perform_grid_search(X_train, y_train)\n",
    "c_random_state = 42\n",
    "print(n_estimator, depth, c_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit a new model with best params, so we can see feature importance\n",
    "rf = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator,\n",
    "                            criterion='entropy', random_state=c_random_state, oob_score=True, class_weight='balanced_subsample')\n",
    "\n",
    "rf.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Sequentially Bootstrapped model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_mat = get_ind_matrix(triple_barrier_events.loc[y_train.index]) # get indicator matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequentially bootstrapped samples it can run for a day, instead of this use already generated samples\n",
    "samples = []\n",
    "num_samples = [None] # none = data set length\n",
    "num_times = [200]\n",
    "for sample_len, n_times in zip(num_samples, num_times):\n",
    "    for i in range(n_times):\n",
    "        subsample = seq_bootstrap(ind_mat,sample_length=sample_len,  compare=False)\n",
    "        samples.append(subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pickle.load(open('bootstapped_sample.pkl', 'rb')) # pregenerated bootstrapped samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate probs for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [1/200 for i in range(200)]\n",
    "sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and cross-validate model with reduced samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "rf_seq = SequentialBootstrappingRandomForestClassifier(criterion='entropy', bootstrapped_events=samples, \n",
    "                                                       draw_subsample_prob=probs, \n",
    "                                                       n_estimators=500, max_depth=5, oob_score=True, n_jobs=-1,\n",
    "                                                      class_weight='balanced_subsample')\n",
    "rf_seq.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_seq.oob_score_, rf.oob_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Metrics (initial model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "y_pred_rf = rf.predict_proba(X_train)[:, 1]\n",
    "y_pred = rf.predict(X_train)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_train, y_pred_rf)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(y_train, y_pred))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Metrics (sequentially bootstrapped model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "y_pred_rf = rf_seq.predict_proba(X_train)[:, 1]\n",
    "y_pred = rf_seq.predict(X_train)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_train, y_pred_rf)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(y_train, y_pred))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Metrics (initial model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-label\n",
    "# Performance Metrics\n",
    "y_pred_rf = rf.predict_proba(X_validate)[:, 1]\n",
    "y_pred = rf.predict(X_validate)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_validate, y_pred_rf)\n",
    "print(classification_report(y_validate, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_validate, y_pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(y_validate, y_pred))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-label\n",
    "# Performance Metrics\n",
    "y_pred_rf = rf_seq.predict_proba(X_validate)[:, 1]\n",
    "y_pred = rf_seq.predict(X_validate)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_validate, y_pred_rf)\n",
    "print(classification_report(y_validate, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_validate, y_pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(y_validate, y_pred))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(rf.predict(X_validate), y_validate), f1_score(rf_seq.predict(X_validate), y_validate))\n",
    "print(precision_score(rf.predict(X_validate), y_validate), precision_score(rf_seq.predict(X_validate), y_validate))\n",
    "print(recall_score(rf.predict(X_validate), y_validate), recall_score(rf_seq.predict(X_validate), y_validate))\n",
    "print(accuracy_score(rf.predict(X_validate), y_validate), accuracy_score(rf_seq.predict(X_validate), y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary model\n",
    "primary_forecast = pd.DataFrame(labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "start = primary_forecast.index.get_loc('2016-01-20 14:42:03.869000')\n",
    "end = primary_forecast.index.get_loc('2017-12-04 15:45:10.747000') + 1\n",
    "\n",
    "subset_prim = primary_forecast[start:end]\n",
    "\n",
    "# Performance Metrics\n",
    "actual = subset_prim['actual']\n",
    "pred = subset_prim['pred']\n",
    "print(classification_report(y_true=actual, y_pred=pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(actual, pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(actual, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "title = 'Feature Importance:'\n",
    "figsize = (15, 5)\n",
    "\n",
    "feat_imp = pd.DataFrame({'Importance':rf.feature_importances_})    \n",
    "feat_imp['feature'] = X.columns\n",
    "feat_imp.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "feat_imp = feat_imp\n",
    "\n",
    "feat_imp.sort_values(by='Importance', inplace=True)\n",
    "feat_imp = feat_imp.set_index('feature', drop=True)\n",
    "feat_imp.plot.barh(title=title, figsize=figsize)\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Performance Tear Sheets (In-sample)\n",
    "\n",
    "### Without Meta Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_returns(intraday_returns):\n",
    "    \"\"\"\n",
    "    This changes returns into daily returns that will work using pyfolio. Its not perfect...\n",
    "    \"\"\"\n",
    "    \n",
    "    cum_rets = ((intraday_returns + 1).cumprod())\n",
    "\n",
    "    # Downsample to daily\n",
    "    daily_rets = cum_rets.resample('B').last()\n",
    "\n",
    "    # Forward fill, Percent Change, Drop NaN\n",
    "    daily_rets = daily_rets.ffill().pct_change().dropna()\n",
    "    \n",
    "    return daily_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dates = X_validate.index\n",
    "base_rets = labels.loc[valid_dates, 'ret']\n",
    "primary_model_rets = get_daily_returns(base_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up the function to extract the KPIs from pyfolio\n",
    "perf_func = pf.timeseries.perf_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the statistics in a dataframe\n",
    "perf_stats_all = perf_func(returns=primary_model_rets, \n",
    "                           factor_returns=None, \n",
    "                           positions=None,\n",
    "                           transactions=None,\n",
    "                           turnover_denom=\"AGB\")\n",
    "perf_stats_df = pd.DataFrame(data=perf_stats_all, columns=['Primary Model'])\n",
    "\n",
    "pf.show_perf_stats(primary_model_rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Meta Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_returns = labels.loc[valid_dates, 'ret'] * y_pred\n",
    "daily_meta_rets = get_daily_returns(meta_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the KPIs in a dataframe\n",
    "perf_stats_all = perf_func(returns=daily_meta_rets, \n",
    "                           factor_returns=None, \n",
    "                           positions=None,\n",
    "                           transactions=None,\n",
    "                           turnover_denom=\"AGB\")\n",
    "\n",
    "perf_stats_df['Meta Model'] = perf_stats_all\n",
    "\n",
    "pf.show_perf_stats(daily_meta_rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Perform out-of-sample test\n",
    "### Meta Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extarct data for out-of-sample (OOS)\n",
    "X_oos = X['2018-01-02':]\n",
    "y_oos = y['2018-01-02':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "y_pred_rf = rf.predict_proba(X_oos)[:, 1]\n",
    "y_pred = rf.predict(X_oos)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_oos, y_pred_rf)\n",
    "print(classification_report(y_oos, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_oos, y_pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(y_oos, y_pred))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequentially bootstrapped model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "y_pred_rf = rf_seq.predict_proba(X_oos)[:, 1]\n",
    "y_pred = rf_seq.predict(X_oos)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_oos, y_pred_rf)\n",
    "print(classification_report(y_oos, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_oos, y_pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(y_oos, y_pred))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(rf.predict(X_oos), y_oos), f1_score(rf_seq.predict(X_oos), y_oos))\n",
    "print(precision_score(rf.predict(X_oos), y_oos), precision_score(rf_seq.predict(X_oos), y_oos))\n",
    "print(recall_score(rf.predict(X_oos), y_oos), recall_score(rf_seq.predict(X_oos), y_oos))\n",
    "print(accuracy_score(rf.predict(X_oos), y_oos), accuracy_score(rf_seq.predict(X_oos), y_oos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary model\n",
    "primary_forecast = pd.DataFrame(labels['bin'])\n",
    "primary_forecast['pred'] = 1\n",
    "primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "subset_prim = primary_forecast['2018-01-02':]\n",
    "\n",
    "# Performance Metrics\n",
    "actual = subset_prim['actual']\n",
    "pred = subset_prim['pred']\n",
    "print(classification_report(y_true=actual, y_pred=pred))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(actual, pred))\n",
    "\n",
    "print('')\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(actual, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Model (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = X_oos.index\n",
    "\n",
    "# Downsample to daily\n",
    "prim_rets_test = labels.loc[test_dates, 'ret']\n",
    "daily_rets_prim = get_daily_returns(prim_rets_test)\n",
    "\n",
    "# Save the statistics in a dataframe\n",
    "perf_stats_all = perf_func(returns=daily_rets_prim, \n",
    "                           factor_returns=None, \n",
    "                           positions=None,\n",
    "                           transactions=None,\n",
    "                           turnover_denom=\"AGB\")\n",
    "\n",
    "perf_stats_df['Primary Model OOS'] = perf_stats_all\n",
    "\n",
    "# pf.create_returns_tear_sheet(labels.loc[test_dates, 'ret'], benchmark_rets=None)\n",
    "pf.show_perf_stats(daily_rets_prim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Model (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_returns = labels.loc[test_dates, 'ret'] * y_pred\n",
    "daily_rets_meta = get_daily_returns(meta_returns)\n",
    "\n",
    "# save the KPIs in a dataframe\n",
    "perf_stats_all = perf_func(returns=daily_rets_meta, \n",
    "                           factor_returns=None, \n",
    "                           positions=None,\n",
    "                           transactions=None,\n",
    "                           turnover_denom=\"AGB\")\n",
    "\n",
    "perf_stats_df['Meta Model OOS'] = perf_stats_all\n",
    "\n",
    "pf.create_returns_tear_sheet(daily_rets_meta, benchmark_rets=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
